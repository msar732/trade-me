

# Advanced AI Engine Implementation - ai_engine/ml_models.py
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, IsolationForest
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout
from django.core.cache import cache
from django.conf import settings
import joblib
import cv2
import tensorflow as tf
from PIL import Image
import torch
from transformers import pipeline, AutoTokenizer, AutoModel
import logging

logger = logging.getLogger(__name__)

class AdvancedRecommendationEngine:
    """
    Next-generation AI recommendation system using hybrid approaches
    """
    
    def __init__(self):
        self.collaborative_model = None
        self.content_model = None
        self.deep_learning_model = None
        self.user_embeddings = {}
        self.item_embeddings = {}
        self.load_or_train_models()
    
    def load_or_train_models(self):
        """Load pre-trained models or train new ones"""
        try:
            # Load collaborative filtering model
            self.collaborative_model = joblib.load(
                settings.AI_MODEL_PATH / 'collaborative_model.pkl'
            )
            
            # Load content-based model
            self.content_model = joblib.load(
                settings.AI_MODEL_PATH / 'content_model.pkl'
            )
            
            # Load deep learning model
            self.deep_learning_model = load_model(
                settings.AI_MODEL_PATH / 'deep_recommendation_model.h5'
            )
            
        except FileNotFoundError:
            logger.info("Models not found, training new models...")
            self.train_models()
    
    def train_models(self):
        """Train recommendation models from scratch"""
        from accounts.models import User
        from listings.models import BaseListing
        from ai_engine.models import UserInteraction
        
        # Get training data
        interactions = UserInteraction.objects.select_related('user').all()
        users = User.objects.all()
        listings = BaseListing.objects.filter(status='active').all()
        
        # Prepare training data
        user_item_matrix = self.create_user_item_matrix(interactions, users, listings)
        
        # Train collaborative filtering model
        self.collaborative_model = self.train_collaborative_filtering(user_item_matrix)
        
        # Train content-based model
        self.content_model = self.train_content_based_model(listings)
        
        # Train deep learning model
        self.deep_learning_model = self.train_deep_learning_model(interactions)
        
        # Save models
        self.save_models()
    
    def create_user_item_matrix(self, interactions, users, listings):
        """Create user-item interaction matrix"""
        user_ids = [user.id for user in users]
        item_ids = [listing.id for listing in listings]
        
        matrix = np.zeros((len(user_ids), len(item_ids)))
        
        for interaction in interactions:
            user_idx = user_ids.index(interaction.user.id)
            item_idx = item_ids.index(interaction.object_id)
            
            # Weight different interaction types
            weight = {
                'view': 1.0,
                'favorite': 3.0,
                'contact': 5.0,
                'purchase': 10.0
            }.get(interaction.interaction_type, 1.0)
            
            matrix[user_idx][item_idx] += weight
        
        return matrix
    
    def train_collaborative_filtering(self, user_item_matrix):
        """Train collaborative filtering using matrix factorization"""
        from sklearn.decomposition import NMF
        
        model = NMF(n_components=50, random_state=42)
        model.fit(user_item_matrix)
        
        return model
    
    def train_content_based_model(self, listings):
        """Train content-based recommendation model"""
        # Extract text features
        texts = [f"{listing.title} {listing.description}" for listing in listings]
        
        vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        text_features = vectorizer.fit_transform(texts)
        
        # Store the vectorizer and features
        model = {
            'vectorizer': vectorizer,
            'features': text_features,
            'listings': [listing.id for listing in listings]
        }
        
        return model
    
    def train_deep_learning_model(self, interactions):
        """Train deep learning model for recommendations"""
        # Prepare sequence data
        user_sequences = {}
        
        for interaction in interactions.order_by('created_at'):
            user_id = interaction.user.id
            if user_id not in user_sequences:
                user_sequences[user_id] = []
            user_sequences[user_id].append(interaction.object_id)
        
        # Create training sequences
        X, y = [], []
        sequence_length = 10
        
        for user_id, sequence in user_sequences.items():
            if len(sequence) > sequence_length:
                for i in range(len(sequence) - sequence_length):
                    X.append(sequence[i:i+sequence_length])
                    y.append(sequence[i+sequence_length])
        
        # Convert to numpy arrays
        X = np.array(X)
        y = np.array(y)
        
        # Build LSTM model
        model = Sequential([
            Embedding(input_dim=100000, output_dim=100, input_length=sequence_length),
            LSTM(128, return_sequences=True),
            Dropout(0.2),
            LSTM(64),
            Dropout(0.2),
            Dense(32, activation='relu'),
            Dense(100000, activation='softmax')
        ])
        
        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
        model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)
        
        return model
    
    def get_recommendations(self, user, limit=10):
        """Get hybrid recommendations for user"""
        cache_key = f'recommendations_user_{user.id}_{limit}'
        cached_result = cache.get(cache_key)
        
        if cached_result:
            return cached_result
        
        # Get recommendations from different models
        collaborative_recs = self.get_collaborative_recommendations(user, limit * 2)
        content_recs = self.get_content_recommendations(user, limit * 2)
        deep_recs = self.get_deep_learning_recommendations(user, limit * 2)
        
        # Combine and rank recommendations
        combined_recs = self.combine_recommendations(
            collaborative_recs, content_recs, deep_recs, limit
        )
        
        # Cache results
        cache.set(cache_key, combined_recs, 3600)  # 1 hour
        
        return combined_recs
    
    def get_collaborative_recommendations(self, user, limit):
        """Get collaborative filtering recommendations"""
        if not self.collaborative_model:
            return []
        
        try:
            # Get user's interaction history
            from ai_engine.models import UserInteraction
            from listings.models import BaseListing
            
            user_interactions = UserInteraction.objects.filter(user=user).values_list('object_id', flat=True)
            
            if not user_interactions:
                return self.get_popular_items(limit)
            
            # Get all available listings
            all_listings = BaseListing.objects.filter(status='active', is_deleted=False)
            listing_ids = list(all_listings.values_list('id', flat=True))
            
            if not listing_ids:
                return []
            
            # Create user vector for prediction
            user_vector = np.zeros(len(listing_ids))
            for interaction_id in user_interactions:
                if interaction_id in listing_ids:
                    idx = listing_ids.index(interaction_id)
                    user_vector[idx] = 1.0
            
            # Get recommendations using the collaborative model
            if hasattr(self.collaborative_model, 'transform'):
                # For NMF model
                user_embedding = self.collaborative_model.transform([user_vector])
                item_embeddings = self.collaborative_model.components_
                
                # Calculate scores
                scores = np.dot(user_embedding, item_embeddings)[0]
            else:
                # For other models, use predict method
                scores = self.collaborative_model.predict([user_vector])[0]
            
            # Get top recommendations
            top_indices = np.argsort(scores)[-limit:][::-1]
            recommended_ids = [listing_ids[i] for i in top_indices if scores[i] > 0]
            
            # Return actual listing objects
            return list(BaseListing.objects.filter(id__in=recommended_ids))
            
        except Exception as e:
            logger.error(f"Error in collaborative recommendations: {e}")
            return self.get_popular_items(limit)
    
    def get_content_recommendations(self, user, limit):
        """Get content-based recommendations"""
        if not self.content_model:
            return []
        
        # Get user's interaction history
        from ai_engine.models import UserInteraction
        user_interactions = UserInteraction.objects.filter(user=user).order_by('-created_at')[:50]
        
        if not user_interactions:
            return self.get_popular_items(limit)
        
        # Find similar items based on content
        similar_items = []
        for interaction in user_interactions:
            similar = self.find_similar_items(interaction.object_id, limit // len(user_interactions))
            similar_items.extend(similar)
        
        return similar_items[:limit]
    
    def get_deep_learning_recommendations(self, user, limit):
        """Get deep learning model recommendations"""
        if not self.deep_learning_model:
            return []
        
        # Get user's recent interaction sequence
        from ai_engine.models import UserInteraction
        recent_interactions = UserInteraction.objects.filter(user=user).order_by('-created_at')[:10]
        
        if len(recent_interactions) < 10:
            return []
        
        # Prepare input sequence
        sequence = [interaction.object_id for interaction in recent_interactions]
        sequence = np.array([sequence])
        
        # Get predictions
        predictions = self.deep_learning_model.predict(sequence)
        top_items = np.argsort(predictions[0])[-limit:][::-1]
        
        return top_items.tolist()
    
    def combine_recommendations(self, collab_recs, content_recs, deep_recs, limit):
        """Combine recommendations from different models"""
        # Weighted combination
        weights = {'collaborative': 0.4, 'content': 0.4, 'deep': 0.2}
        
        item_scores = {}
        
        # Score items from collaborative filtering
        for i, item in enumerate(collab_recs):
            score = (len(collab_recs) - i) / len(collab_recs)
            item_scores[item] = item_scores.get(item, 0) + score * weights['collaborative']
        
        # Score items from content-based
        for i, item in enumerate(content_recs):
            score = (len(content_recs) - i) / len(content_recs)
            item_scores[item] = item_scores.get(item, 0) + score * weights['content']
        
        # Score items from deep learning
        for i, item in enumerate(deep_recs):
            score = (len(deep_recs) - i) / len(deep_recs)
            item_scores[item] = item_scores.get(item, 0) + score * weights['deep']
        
        # Sort by combined score
        sorted_items = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)
        
        return [item[0] for item in sorted_items[:limit]]
    
    def save_models(self):
        """Save trained models"""
        import os
        os.makedirs(settings.AI_MODEL_PATH, exist_ok=True)
        
        joblib.dump(self.collaborative_model, settings.AI_MODEL_PATH / 'collaborative_model.pkl')
        joblib.dump(self.content_model, settings.AI_MODEL_PATH / 'content_model.pkl')
        self.deep_learning_model.save(settings.AI_MODEL_PATH / 'deep_recommendation_model.h5')
    
    def find_similar_items(self, item_id, limit):
        """Find similar items based on content similarity"""
        try:
            from listings.models import BaseListing
            
            # Get the target item
            target_item = BaseListing.objects.get(id=item_id)
            target_text = f"{target_item.title} {target_item.description}"
            
            # Get other items in the same category
            similar_items = BaseListing.objects.filter(
                category=target_item.category,
                status='active',
                is_deleted=False
            ).exclude(id=item_id)[:100]  # Limit for performance
            
            if not similar_items:
                return []
            
            # Calculate similarity scores
            similar_texts = [f"{item.title} {item.description}" for item in similar_items]
            all_texts = [target_text] + similar_texts
            
            # Use TF-IDF for text similarity
            vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
            tfidf_matrix = vectorizer.fit_transform(all_texts)
            
            # Calculate cosine similarity
            similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()
            
            # Get top similar items
            top_indices = np.argsort(similarities)[-limit:][::-1]
            return [similar_items[i] for i in top_indices if similarities[i] > 0.1]
            
        except Exception as e:
            logger.error(f"Error finding similar items: {e}")
            return []
    
    def get_popular_items(self, limit):
        """Get popular items based on interaction count"""
        try:
            from listings.models import BaseListing
            from ai_engine.models import UserInteraction
            
            # Get items with most interactions
            popular_items = BaseListing.objects.filter(
                status='active',
                is_deleted=False
            ).annotate(
                interaction_count=models.Count('userinteraction')
            ).order_by('-interaction_count', '-created_at')[:limit]
            
            return list(popular_items)
            
        except Exception as e:
            logger.error(f"Error getting popular items: {e}")
            return []

class AdvancedFraudDetectionSystem:
    """
    Multi-layered fraud detection using various AI techniques
    """
    
    def __init__(self):
        self.text_analyzer = None
        self.image_analyzer = None
        self.behavior_analyzer = None
        self.ensemble_model = None
        self.load_models()
    
    def load_models(self):
        """Load fraud detection models"""
        try:
            # Load pre-trained models
            self.text_analyzer = pipeline("text-classification", 
                model="distilbert-base-uncased-finetuned-sst-2-english")
            
            # Load custom models
            self.behavior_analyzer = joblib.load(
                settings.AI_MODEL_PATH / 'behavior_fraud_model.pkl'
            )
            
        except Exception as e:
            logger.warning(f"Could not load fraud detection models: {e}")
            self.initialize_basic_models()
    
    def initialize_basic_models(self):
        """Initialize basic fraud detection models"""
        # Initialize with basic rule-based detection
        self.ensemble_model = IsolationForest(contamination=0.1, random_state=42)
    
    def analyze_listing(self, listing):
        """Comprehensive fraud analysis of a listing"""
        fraud_scores = {}
        
        # Text analysis
        fraud_scores['text'] = self.analyze_text_fraud(listing.title, listing.description)
        
        # Image analysis
        if hasattr(listing, 'images') and listing.images.exists():
            fraud_scores['image'] = self.analyze_image_fraud(listing.images.all())
        else:
            fraud_scores['image'] = 0.5  # No images is suspicious
        
        # Price analysis
        fraud_scores['price'] = self.analyze_price_fraud(listing)
        
        # User behavior analysis
        fraud_scores['behavior'] = self.analyze_user_behavior(listing.user)
        
        # Location analysis
        fraud_scores['location'] = self.analyze_location_fraud(listing)
        
        # Combine scores
        overall_score = self.combine_fraud_scores(fraud_scores)
        
        return {
            'overall_score': overall_score,
            'individual_scores': fraud_scores,
            'is_suspicious': overall_score > 0.7,
            'confidence': min(overall_score * 1.2, 1.0)
        }
    
    def analyze_text_fraud(self, title, description):
        """Analyze text for fraud indicators"""
        text = f"{title} {description}".lower()
        
        # Rule-based indicators
        fraud_keywords = [
            'urgent', 'limited time', 'act now', 'guaranteed profit',
            'no questions asked', 'cash only', 'must sell today',
            'too good to be true', 'once in lifetime', 'exclusive deal'
        ]
        
        keyword_score = sum(1 for keyword in fraud_keywords if keyword in text) / len(fraud_keywords)
        
        # Text quality analysis
        if len(description) < 50:
            quality_score = 0.8  # Very short descriptions are suspicious
        elif len(description) > 2000:
            quality_score = 0.6  # Extremely long descriptions can be spam
        else:
            quality_score = 0.2
        
        # Grammar and spelling check (simplified)
        if text.count('!') > 5 or text.isupper():
            grammar_score = 0.7
        else:
            grammar_score = 0.1
        
        return (keyword_score + quality_score + grammar_score) / 3
    
    def analyze_image_fraud(self, images):
        """Analyze images for fraud indicators"""
        if not images:
            return 0.8  # No images is highly suspicious
        
        fraud_indicators = []
        
        for image in images:
            try:
                # Load image
                img_path = image.image.path
                img = cv2.imread(img_path)
                
                if img is None:
                    fraud_indicators.append(0.9)
                    continue
                
                # Check image quality
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
                
                if laplacian_var < 100:  # Blurry image
                    fraud_indicators.append(0.6)
                else:
                    fraud_indicators.append(0.2)
                
                # Check for watermarks (simplified)
                if self.detect_watermarks(img):
                    fraud_indicators.append(0.8)
                
                # Check image metadata for manipulation
                if self.check_image_manipulation(img_path):
                    fraud_indicators.append(0.7)
                
            except Exception as e:
                logger.warning(f"Error analyzing image: {e}")
                fraud_indicators.append(0.5)
        
        return sum(fraud_indicators) / len(fraud_indicators) if fraud_indicators else 0.5
    
    def detect_watermarks(self, image):
        """Simple watermark detection"""
        try:
            # Convert to grayscale
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # Apply edge detection
            edges = cv2.Canny(gray, 50, 150)
            
            # Detect text regions using morphological operations
            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
            dilated = cv2.dilate(edges, kernel, iterations=2)
            
            # Find contours
            contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # Analyze contours for watermark characteristics
            watermark_score = 0.0
            total_area = image.shape[0] * image.shape[1]
            
            for contour in contours:
                area = cv2.contourArea(contour)
                if area > 100:  # Filter small noise
                    # Check aspect ratio (watermarks are often rectangular)
                    x, y, w, h = cv2.boundingRect(contour)
                    aspect_ratio = w / h if h > 0 else 0
                    
                    # Check if it's in typical watermark positions (corners, center-bottom)
                    center_x, center_y = x + w//2, y + h//2
                    img_h, img_w = image.shape[:2]
                    
                    # Check if in corner regions or bottom center
                    is_corner = (center_x < img_w * 0.2 or center_x > img_w * 0.8) and \
                               (center_y < img_h * 0.2 or center_y > img_h * 0.8)
                    is_bottom_center = img_w * 0.3 < center_x < img_w * 0.7 and center_y > img_h * 0.7
                    
                    if is_corner or is_bottom_center:
                        # Calculate watermark likelihood based on area and position
                        area_ratio = area / total_area
                        if 0.001 < area_ratio < 0.1:  # Reasonable watermark size
                            watermark_score += area_ratio * 2
                            
                            # Bonus for text-like aspect ratios
                            if 2 < aspect_ratio < 8:  # Text-like rectangles
                                watermark_score += 0.1
            
            # Normalize score to 0-1 range
            return min(watermark_score, 1.0)
            
        except Exception as e:
            logger.error(f"Error in watermark detection: {e}")
            return 0.5  # Neutral score on error
    
    def check_image_manipulation(self, image_path):
        """Check for image manipulation indicators"""
        try:
            from PIL import Image
            from PIL.ExifTags import TAGS
            
            image = Image.open(image_path)
            
            # Check EXIF data
            exif_data = image._getexif()
            
            if not exif_data:
                return True  # No EXIF data is suspicious for real photos
            
            # Check for software manipulation indicators
            for tag_id, value in exif_data.items():
                tag = TAGS.get(tag_id, tag_id)
                if tag == "Software" and any(editor in str(value).lower() 
                    for editor in ['photoshop', 'gimp', 'paint.net']):
                    return True
            
            return False
            
        except Exception:
            return False
    
    def analyze_price_fraud(self, listing):
        """Analyze pricing for fraud indicators"""
        try:
            # Get similar listings for comparison
            similar_listings = self.get_similar_listings(listing)
            
            if not similar_listings:
                return 0.3  # No comparison data
            
            prices = [float(l.price) for l in similar_listings]
            avg_price = sum(prices) / len(prices)
            
            price_ratio = float(listing.price) / avg_price
            
            # Very low prices compared to market are suspicious
            if price_ratio < 0.5:
                return 0.9
            elif price_ratio < 0.7:
                return 0.6
            # Very high prices might also be suspicious
            elif price_ratio > 3.0:
                return 0.7
            else:
                return 0.1
                
        except Exception:
            return 0.3
    
    def analyze_user_behavior(self, user):
        """Analyze user behavior patterns for fraud"""
        from ai_engine.models import UserInteraction
        
        # Check account age
        account_age = (timezone.now() - user.created_at).days
        if account_age < 7:
            age_score = 0.8
        elif account_age < 30:
            age_score = 0.4
        else:
            age_score = 0.1
        
        # Check posting frequency
        user_listings = user.baselisting_listings.count()
        if user_listings > 50 and account_age < 30:
            frequency_score = 0.9  # Too many listings too quickly
        else:
            frequency_score = 0.1
        
        # Check interaction patterns
        interactions = UserInteraction.objects.filter(user=user).count()
        if user_listings > 0 and interactions == 0:
            interaction_score = 0.7  # Only posts, never browses
        else:
            interaction_score = 0.1
        
        return (age_score + frequency_score + interaction_score) / 3
    
    def analyze_location_fraud(self, listing):
        """Analyze location consistency"""
        # Check if location matches user's registered location
        if hasattr(listing.user, 'location') and listing.user.location:
            if listing.location.distance(listing.user.location) > 1000:  # 1000km away
                return 0.6
        
        # Check for GPS coordinate consistency
        if listing.coordinates:
            # Verify coordinates match the address (simplified)
            return 0.1
        else:
            return 0.4  # No coordinates provided
    
    def combine_fraud_scores(self, fraud_scores):
        """Combine individual fraud scores into overall score"""
        weights = {
            'text': 0.25,
            'image': 0.25,
            'price': 0.2,
            'behavior': 0.2,
            'location': 0.1
        }
        
        overall = sum(score * weights[category] for category, score in fraud_scores.items())
        return min(overall, 1.0)
    
    def get_similar_listings(self, listing):
        """Get similar listings for comparison"""
        from listings.models import BaseListing
        
        similar = BaseListing.objects.filter(
            category=listing.category,
            status='active'
        ).exclude(id=listing.id)[:10]
        
        return similar

class AdvancedPricePredictionEngine:
    """
    AI-powered price prediction using multiple algorithms
    """
    
    def __init__(self):
        self.category_models = {}
        self.load_models()
    
    def load_models(self):
        """Load category-specific pricing models"""
        categories = ['motors', 'property', 'electronics', 'fashion']
        
        for category in categories:
            try:
                model_path = settings.AI_MODEL_PATH / f'price_model_{category}.pkl'
                self.category_models[category] = joblib.load(model_path)
            except FileNotFoundError:
                logger.info(f"Training new price model for {category}")
                self.train_category_model(category)
    
    def train_category_model(self, category):
        """Train price prediction model for specific category"""
        # Get training data for the category
        training_data = self.prepare_training_data(category)
        
        if training_data.empty:
            logger.warning(f"No training data for category: {category}")
            return
        
        # Feature engineering
        features = self.extract_features(training_data)
        target = training_data['price']
        
        # Train ensemble model
        model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
        
        model.fit(features, target)
        
        # Save model
        import os
        os.makedirs(settings.AI_MODEL_PATH, exist_ok=True)
        joblib.dump(model, settings.AI_MODEL_PATH / f'price_model_{category}.pkl')
        
        self.category_models[category] = model
    
    def prepare_training_data(self, category):
        """Prepare training data for price prediction"""
        from listings.models import BaseListing
        
        # Get listings for the category
        listings = BaseListing.objects.filter(
            category__slug=category,
            status__in=['active', 'sold']
        ).values(
            'price', 'condition', 'created_at', 'view_count',
            'location', 'user__trust_score'
        )
        
        return pd.DataFrame(listings)
    
    def extract_features(self, data):
        """Extract features for price prediction"""
        features = pd.DataFrame()
        
        # Basic features
        features['condition_score'] = data['condition'].map({
            'new': 1.0, 'excellent': 0.9, 'very_good': 0.8,
            'good': 0.7, 'fair': 0.5, 'poor': 0.3
        })
        
        features['age_days'] = (pd.Timestamp.now() - pd.to_datetime(data['created_at'])).dt.days
        features['view_count'] = data['view_count']
        features['seller_trust'] = data['user__trust_score']
        
        # Handle missing values
        features = features.fillna(features.mean())
        
        return features
    
    def predict_price(self, listing):
        """Predict optimal price for a listing"""
        category = listing.category.slug
        
        if category not in self.category_models:
            return self.get_fallback_prediction(listing)
        
        model = self.category_models[category]
        
        # Extract features for prediction
        features = self.extract_listing_features(listing)
        
        # Make prediction
        predicted_price = model.predict([features])[0]
        
        # Get confidence interval
        confidence = self.calculate_confidence(model, features)
        
        return {
            'predicted_price': max(0, predicted_price),
            'confidence': confidence,
            'price_range': {
                'min': predicted_price * 0.9,
                'max': predicted_price * 1.1
            },
            'market_analysis': self.get_market_analysis(listing)
        }
    
    def extract_listing_features(self, listing):
        """Extract features from a listing for prediction"""
        features = []
        
        # Condition score
        condition_map = {
            'new': 1.0, 'excellent': 0.9, 'very_good': 0.8,
            'good': 0.7, 'fair': 0.5, 'poor': 0.3
        }
        features.append(condition_map.get(listing.condition, 0.5))
        
        # Age (for existing listings)
        if hasattr(listing, 'created_at'):
            age_days = (timezone.now() - listing.created_at).days
            features.append(age_days)
        else:
            features.append(0)
        
        # View count
        features.append(getattr(listing, 'view_count', 0))
        
        # Seller trust score
        features.append(getattr(listing.user, 'trust_score', 50.0))
        
        return features
    
    def calculate_confidence(self, model, features):
        """Calculate prediction confidence"""
        # Use the variance of predictions from different trees
        if hasattr(model, 'estimators_'):
            predictions = [tree.predict([features])[0] for tree in model.estimators_]
            variance = np.var(predictions)
            confidence = max(0.1, min(0.9, 1.0 - variance / np.mean(predictions)))
        else:
            confidence = 0.5
        
        return confidence
    
    def get_market_analysis(self, listing):
        """Get market analysis for the listing"""
        from listings.models import BaseListing
        
        # Get similar listings
        similar_listings = BaseListing.objects.filter(
            category=listing.category,
            status='active'
        ).exclude(id=listing.id if hasattr(listing, 'id') else None)
        
        if similar_listings.exists():
            prices = [float(l.price) for l in similar_listings[:50]]
            
            return {
                'market_average': sum(prices) / len(prices),
                'market_median': sorted(prices)[len(prices) // 2],
                'price_trend': 'stable',  # Simplified
                'competition_level': len(prices)
            }
        
        return {
            'market_average': 0,
            'market_median': 0,
            'price_trend': 'unknown',
            'competition_level': 0
        }
    
    def get_fallback_prediction(self, listing):
        """Fallback prediction when no model is available"""
        return {
            'predicted_price': float(listing.price) if hasattr(listing, 'price') else 0,
            'confidence': 0.3,
            'price_range': {'min': 0, 'max': 0},
            'market_analysis': self.get_market_analysis(listing)
        }

# Enhanced Celery Tasks - ai_engine/tasks.py
from celery import shared_task
from django.utils import timezone
from .ml_models import (
    AdvancedRecommendationEngine, 
    AdvancedFraudDetectionSystem, 
    AdvancedPricePredictionEngine
)
import logging

logger = logging.getLogger(__name__)

@shared_task(bind=True, max_retries=3)
def analyze_listing_with_ai(self, listing_id, listing_type='BaseListing'):
    """
    Comprehensive AI analysis of a listing
    """
    try:
        # Import the appropriate model based on listing type
        if listing_type == 'MotorListing':
            from motors.models import MotorListing as ListingModel
        elif listing_type == 'PropertyListing':
            from property.models import PropertyListing as ListingModel
        else:
            from listings.models import BaseListing as ListingModel
        
        listing = ListingModel.objects.get(id=listing_id)
        
        # Initialize AI engines
        fraud_detector = AdvancedFraudDetectionSystem()
        price_predictor = AdvancedPricePredictionEngine()
        
        # Fraud analysis
        fraud_analysis = fraud_detector.analyze_listing(listing)
        
        # Price analysis
        price_analysis = price_predictor.predict_price(listing)
        
        # Update listing with AI results
        listing.fraud_score = fraud_analysis['overall_score']
        listing.ai_score = calculate_overall_ai_score(fraud_analysis, price_analysis)
        listing.is_ai_verified = fraud_analysis['overall_score'] < 0.3
        listing.ai_analysis = {
            'fraud_analysis': fraud_analysis,
            'price_analysis': price_analysis,
            'analyzed_at': timezone.now().isoformat()
        }
        
        listing.save()
        
        # Log analysis results
        logger.info(f"AI analysis completed for listing {listing_id}")
        
        return {
            'success': True,
            'listing_id': listing_id,
            'ai_score': listing.ai_score,
            'fraud_score': listing.fraud_score
        }
        
    except Exception as exc:
        logger.error(f"AI analysis failed for listing {listing_id}: {exc}")
        
        # Retry with exponential backoff
        raise self.retry(exc=exc, countdown=60 * (2 ** self.request.retries))

@shared_task
def update_user_recommendations(user_id):
    """
    Update AI recommendations for a user
    """
    try:
        from accounts.models import User
        user = User.objects.get(id=user_id)
        
        recommendation_engine = AdvancedRecommendationEngine()
        recommendations = recommendation_engine.get_recommendations(user, limit=50)
        
        # Store recommendations in cache and database
        from django.core.cache import cache
        cache.set(f'user_recommendations_{user_id}', recommendations, 3600 * 24)  # 24 hours
        
        logger.info(f"Updated recommendations for user {user_id}")
        
        return {
            'success': True,
            'user_id': user_id,
            'recommendation_count': len(recommendations)
        }
        
    except Exception as exc:
        logger.error(f"Failed to update recommendations for user {user_id}: {exc}")
        return {'success': False, 'error': str(exc)}

@shared_task
def retrain_ai_models():
    """
    Periodic retraining of AI models with new data
    """
    try:
        logger.info("Starting AI model retraining...")
        
        # Retrain recommendation engine
        recommendation_engine = AdvancedRecommendationEngine()
        recommendation_engine.train_models()
        
        # Retrain fraud detection system
        fraud_detector = AdvancedFraudDetectionSystem()
        # fraud_detector.retrain_models()  # Would implement if needed
        
        # Retrain price prediction models
        price_predictor = AdvancedPricePredictionEngine()
        categories = ['motors', 'property', 'electronics', 'fashion']
        for category in categories:
            price_predictor.train_category_model(category)
        
        logger.info("AI model retraining completed successfully")
        
    except Exception as exc:
        logger.error(f"AI model retraining failed: {exc}")
        return {'success': False, 'error': str(exc)}
    
    return {'success': True, 'message': 'AI models retrained successfully'}
